# analysis/message_parser.py
import re
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class ParseResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–æ–æ–±—â–µ–Ω–∏—è"""
    success: bool
    signal_data: Optional[Dict] = None
    error: Optional[str] = None
    confidence: float = 0.0

class MessageParser:
    """–ü—Ä–æ—Å—Ç–æ–π –∏ —á–µ—Ç–∫–∏–π –ø–∞—Ä—Å–µ—Ä —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤"""
    
    VERSION = "2.0.0"
    
    def __init__(self):
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ç–∏–∫–µ—Ä–æ–≤ - –ø—Ä–æ—Å—Ç—ã–µ –∏ —á–µ—Ç–∫–∏–µ
        self.ticker_patterns = [
            r':\s*([A-Z]{3,6})\b',         # ": SPBE" - –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω
            r'\$([A-Z]{3,6})\b',           # "$SBER"
            r'\b([A-Z]{3,6})\b(?=\s|$)',   # "SBER " - –æ—Ç–¥–µ–ª—å–Ω–æ —Å—Ç–æ—è—â–∏–π
        ]
        
        # –ü–†–û–°–¢–´–ï –ø—Ä–∞–≤–∏–ª–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        self.entry_patterns = {
            'long': [
                r'(?i)\b(–≤—Ö–æ–¥|–∫—É–ø–∏–ª|–ø–æ–∫—É–ø–∫|buy|–Ω–∞–±—Ä–∞–ª)\s+–ª–æ–Ω–≥\b',
                r'(?i)\b(–æ—Ç–∫—Ä—ã–ª|–≤–∑—è–ª)\s+–ª–æ–Ω–≥\b',
                r'(?i)\b(–ª–æ–Ω–≥|long)\s+(–ø–æ|–æ—Ç|–≤|@)',  # "–ª–æ–Ω–≥ –ø–æ —Ü–µ–Ω–µ"
            ],
            'short': [
                r'(?i)\b(–≤—Ö–æ–¥|–ø—Ä–æ–¥–∞–ª|–ø—Ä–æ–¥–∞–∂|sell|–Ω–∞–±—Ä–∞–ª)\s+—à–æ—Ä—Ç\b',
                r'(?i)\b(–æ—Ç–∫—Ä—ã–ª|–≤–∑—è–ª)\s+—à–æ—Ä—Ç\b',
                r'(?i)\b(—à–æ—Ä—Ç|short)\s+(–ø–æ|–æ—Ç|–≤|@)',  # "—à–æ—Ä—Ç –ø–æ —Ü–µ–Ω–µ"
            ]
        }
        
        # –ß–ï–¢–ö–ò–ï –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –≤—ã—Ö–æ–¥–∞/–∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π
        self.exit_patterns = [
            r'(?i)\b(—Å–æ–∫—Ä–∞—Ç–∏–ª|—É–º–µ–Ω—å—à–∏–ª|reduce)\s+(–ª–æ–Ω–≥|—à–æ—Ä—Ç|long|short)\b',  # "—Å–æ–∫—Ä–∞—Ç–∏–ª –ª–æ–Ω–≥"
            r'(?i)\b(—É–≤–µ–ª–∏—á–∏–ª|–¥–æ–±–∞–≤–∏–ª|add)\s+(–ª–æ–Ω–≥|—à–æ—Ä—Ç|long|short)\b',      # "—É–≤–µ–ª–∏—á–∏–ª –ª–æ–Ω–≥"
            r'(?i)\b(–∑–∞–∫—Ä—ã–ª|—Ñ–∏–∫—Å|–≤–∑—è–ª|close)\s*(–ª–æ–Ω–≥|—à–æ—Ä—Ç|long|short)?\b',   # "–∑–∞–∫—Ä—ã–ª –ª–æ–Ω–≥"
            r'(?i)\b(–≤—ã—Ö–æ–¥|exit)\s*(–∏–∑)?\s*(–ª–æ–Ω–≥|—à–æ—Ä—Ç|long|short)?\b',       # "–≤—ã—Ö–æ–¥ –∏–∑ –ª–æ–Ω–≥–∞"
            r'(?i)\b(—Å—Ç–æ–ø|stop)\s*(–ø–æ)?\s*(–ª–æ–Ω–≥|—à–æ—Ä—Ç|long|short)?\b',        # "—Å—Ç–æ–ø –ø–æ –ª–æ–Ω–≥—É"
            r'(?i)(–ª–æ–Ω–≥|—à–æ—Ä—Ç|long|short)\s*üêÉ\s*:',                          # "–ª–æ–Ω–≥üêÉ:"
            r'(?i)(–ª–æ–Ω–≥|—à–æ—Ä—Ç|long|short)\s*üêª\s*:',                          # "—à–æ—Ä—Çüêª:"
        ]
        
        # –ü—Ä–æ—Å—Ç—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        self.trading_keywords = [
            r'(?i)\b(—Å–¥–µ–ª–∫–∞|–ø–æ–∑–∏—Ü–∏—è|—Å–∏–≥–Ω–∞–ª)\b',
            r'(?i)\b(–ª–æ–Ω–≥|—à–æ—Ä—Ç|long|short)\b',
            r'(?i)\b(—Å–æ–∫—Ä–∞—Ç–∏–ª|—É–≤–µ–ª–∏—á–∏–ª|–∑–∞–∫—Ä—ã–ª|–æ—Ç–∫—Ä—ã–ª)\b',
            r'(?i)\b(–∫—É–ø–∏–ª|–ø—Ä–æ–¥–∞–ª|buy|sell)\b',
        ]
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∞–≤—Ç–æ—Ä–∞
        self.author_patterns = [
            r'#([A-Za-z0-9_]+)\s*[-‚Äì]',    # "#ProfitKing -" –∏–ª–∏ "#ProfitKing ‚Äì"
            r'#([A-Za-z0-9_]+)\b',         # –ø—Ä–æ—Å—Ç–æ "#ProfitKing"
        ]
    
    def parse_raw_message(self, raw_message: Dict) -> ParseResult:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        try:
            text = raw_message.get('text', '')
            if not text or not text.strip():
                return ParseResult(success=False, error="Empty message text")
            
            # –û—á–∏—â–∞–µ–º –º—É—Å–æ—Ä
            cleaned_text = self._clean_message_text(text)
            logger.debug(f"Cleaned text: {cleaned_text}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—Ä–≥–æ–≤–æ–µ –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ
            if not self._is_trading_message(cleaned_text):
                return ParseResult(success=False, error="Not a trading message")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
            ticker = self._extract_ticker(cleaned_text)
            if not ticker:
                return ParseResult(success=False, error="No ticker found")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ–ø–µ—Ä–∞—Ü–∏–∏ –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            operation_type, direction = self._analyze_operation(cleaned_text)
            
            author = self._extract_author(cleaned_text, raw_message.get('author_username'))
            prices = self._extract_prices(cleaned_text)
            confidence = self._calculate_confidence(cleaned_text, ticker, direction, operation_type)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            signal_data = {
                'raw_message_id': raw_message['id'],
                'parser_version': self.VERSION,
                'timestamp': raw_message['timestamp'],
                'channel_id': raw_message['channel_id'],
                'author': author,
                'original_text': text,
                'ticker': ticker,
                'direction': direction,
                'signal_type': operation_type,
                'target_price': prices.get('target'),
                'stop_loss': prices.get('stop_loss'),
                'take_profit': prices.get('take_profit'),
                'confidence_score': confidence,
                'extracted_data': {
                    'cleaned_text': cleaned_text,
                    'operation_analysis': self._debug_operation_analysis(cleaned_text),
                    'all_tickers': self._extract_all_tickers(cleaned_text),
                    'all_prices': self._extract_all_numbers(cleaned_text),
                    'raw_message_id': raw_message['id']
                }
            }
            
            logger.info(f"‚úÖ Parsed message {raw_message['id']}: "
                       f"ticker={ticker}, direction={direction}, operation={operation_type}, confidence={confidence}")
            
            return ParseResult(success=True, signal_data=signal_data, confidence=confidence)
            
        except Exception as e:
            logger.error(f"‚ùå Error parsing message {raw_message.get('id')}: {e}")
            return ParseResult(success=False, error=f"Exception: {str(e)}")
    
    def _clean_message_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –º—É—Å–æ—Ä–∞ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        cleaned = text
        
        # –£–¥–∞–ª—è–µ–º –º—É—Å–æ—Ä–Ω—ã–µ —á–∞—Å—Ç–∏
        garbage_patterns = [
            r'–ë–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.*?(?=\n|$)',          # "–ë–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏..."
            r'üëâ\[@–∫–æ–ø–∏–∏.*?\].*?(?=\n|$)',           # –°—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–ø–∏–∏
            r'\[.*?\]\(https://t\.me/.*?\)',         # Markdown —Å—Å—ã–ª–∫–∏
            r'https://t\.me/\S+',                    # –ü—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏
            r'@\w+_?bot\S*',                         # –ë–æ—Ç—ã
        ]
        
        for pattern in garbage_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        cleaned = re.sub(r'\n\s*\n', '\n', cleaned)
        cleaned = cleaned.strip()
        
        return cleaned
    
    def _is_trading_message(self, text: str) -> bool:
        """–ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–æ—Ä–≥–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"""
        # –ï—Å—Ç—å —Ç–æ—Ä–≥–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞?
        has_keywords = any(re.search(pattern, text) for pattern in self.trading_keywords)
        
        # –ï—Å—Ç—å —Ç–∏–∫–µ—Ä?
        has_ticker = any(re.search(pattern, text) for pattern in self.ticker_patterns)
        
        # –ï—Å—Ç—å —Ç–æ—Ä–≥–æ–≤—ã–µ —ç–º–æ–¥–∑–∏?
        trading_emojis = ['üêÉ', 'üêª', 'üìà', 'üìâ', '‚≠êÔ∏è']
        has_emoji = any(emoji in text for emoji in trading_emojis)
        
        result = has_keywords or has_ticker or has_emoji
        
        logger.debug(f"Trading check: keywords={has_keywords}, ticker={has_ticker}, "
                    f"emoji={has_emoji} -> {result}")
        
        return result
    
    def _extract_ticker(self, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∏–∫–µ—Ä–∞"""
        for pattern in self.ticker_patterns:
            match = re.search(pattern, text)
            if match:
                ticker = match.group(1).upper()
                if 3 <= len(ticker) <= 6 and ticker.isalpha():
                    # –ò—Å–∫–ª—é—á–∞–µ–º –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è
                    if ticker not in ['VIP', 'BOT', 'NEW', 'TOP', 'WIN', 'BUY', 'SELL']:
                        logger.debug(f"Found ticker: {ticker}")
                        return ticker
        return None
    
    def _extract_all_tickers(self, text: str) -> List[str]:
        """–í—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–∏–∫–µ—Ä—ã"""
        tickers = set()
        for pattern in self.ticker_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                ticker = match.upper()
                if 3 <= len(ticker) <= 6 and ticker.isalpha():
                    if ticker not in ['VIP', 'BOT', 'NEW', 'TOP', 'WIN', 'BUY', 'SELL']:
                        tickers.add(ticker)
        return list(tickers)
    
    def _analyze_operation(self, text: str) -> Tuple[str, str]:
        """
        –ê–Ω–∞–ª–∏–∑ –æ–ø–µ—Ä–∞—Ü–∏–∏ - –ö–õ–Æ–ß–ï–í–ê–Ø –õ–û–ì–ò–ö–ê
        
        Returns:
            Tuple[operation_type, direction] –≥–¥–µ:
            operation_type: 'entry' | 'exit' | 'update'
            direction: 'long' | 'short' | 'mixed'
        """
        
        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º EXIT/UPDATE –æ–ø–µ—Ä–∞—Ü–∏–∏ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç!)
        for pattern in self.exit_patterns:
            match = re.search(pattern, text)
            if match:
                logger.debug(f"Found exit pattern: {pattern} -> {match.group()}")
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                if re.search(r'(?i)(–ª–æ–Ω–≥|long)', match.group()):
                    return 'exit', 'long'  # –°–æ–∫—Ä–∞—Ç–∏–ª –ª–æ–Ω–≥ = –ø—Ä–æ–¥–∞–ª –¥–ª–∏–Ω–Ω—É—é –ø–æ–∑–∏—Ü–∏—é
                elif re.search(r'(?i)(—à–æ—Ä—Ç|short)', match.group()):
                    return 'exit', 'short'  # –°–æ–∫—Ä–∞—Ç–∏–ª —à–æ—Ä—Ç = –∑–∞–∫—Ä—ã–ª –∫–æ—Ä–æ—Ç–∫—É—é –ø–æ–∑–∏—Ü–∏—é
                else:
                    return 'exit', 'mixed'  # –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        
        # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º ENTRY –æ–ø–µ—Ä–∞—Ü–∏–∏
        for direction, patterns in self.entry_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text):
                    logger.debug(f"Found entry pattern for {direction}: {pattern}")
                    return 'entry', direction
        
        # 3. –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –Ω–æ –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –ª–æ–Ω–≥/—à–æ—Ä—Ç
        if re.search(r'(?i)\b(–ª–æ–Ω–≥|long)\b', text):
            return 'entry', 'long'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º –≤—Ö–æ–¥–æ–º
        elif re.search(r'(?i)\b(—à–æ—Ä—Ç|short)\b', text):
            return 'entry', 'short'
        
        # 4. –°–æ–≤—Å–µ–º –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏
        return 'entry', 'mixed'
    
    def _debug_operation_analysis(self, text: str) -> Dict:
        """–û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ–ø–µ—Ä–∞—Ü–∏–π"""
        debug_info = {
            'exit_matches': [],
            'entry_matches': [],
            'direction_words': []
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º exit –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        for pattern in self.exit_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                debug_info['exit_matches'].append({'pattern': pattern, 'matches': matches})
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º entry –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        for direction, patterns in self.entry_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                if matches:
                    debug_info['entry_matches'].append({
                        'direction': direction, 
                        'pattern': pattern, 
                        'matches': matches
                    })
        
        # –ò—â–µ–º —Å–ª–æ–≤–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        direction_words = re.findall(r'(?i)\b(–ª–æ–Ω–≥|—à–æ—Ä—Ç|long|short)\b', text)
        debug_info['direction_words'] = direction_words
        
        return debug_info
    
    def _extract_author(self, text: str, fallback: Optional[str] = None) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–∞"""
        for pattern in self.author_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)
        return fallback or 'Unknown'
    
    def _extract_prices(self, text: str) -> Dict[str, Optional[float]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω (–ø–æ–∫–∞ –ø—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è)"""
        prices = {'target': None, 'stop_loss': None, 'take_profit': None}
        
        # –ò—â–µ–º —á–∏—Å–ª–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        price_patterns = {
            'target': [
                r'(?:—Ü–µ–ª|target|—Ç–∞—Ä–≥–µ—Ç|@)\s*:?\s*(\d+(?:[.,]\d+)?)',
                r'(?:–ø–æ|–æ—Ç)\s+(\d+(?:[.,]\d+)?)',
            ],
            'stop_loss': [
                r'(?:—Å—Ç–æ–ø|stop)\s*:?\s*(\d+(?:[.,]\d+)?)',
            ],
            'take_profit': [
                r'(?:—Ç–µ–π–∫|take|–ø—Ä–æ—Ñ–∏—Ç)\s*:?\s*(\d+(?:[.,]\d+)?)',
            ]
        }
        
        for price_type, patterns in price_patterns.items():
            for pattern in patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    try:
                        price = float(match.group(1).replace(',', '.'))
                        if 0.01 <= price <= 100000:
                            prices[price_type] = price
                            break
                    except ValueError:
                        continue
        
        return prices
    
    def _extract_all_numbers(self, text: str) -> List[float]:
        """–í—Å–µ —á–∏—Å–ª–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        numbers = []
        for match in re.finditer(r'\d+(?:[.,]\d+)?', text):
            try:
                number = float(match.group().replace(',', '.'))
                if 0.01 <= number <= 100000:
                    numbers.append(number)
            except ValueError:
                continue
        return numbers
    
    def _calculate_confidence(self, text: str, ticker: str, direction: str, operation: str) -> float:
        """–†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        confidence = 0.0
        
        # –ë–∞–∑–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        if ticker:
            confidence += 0.4
        if direction and direction != 'mixed':
            confidence += 0.3
        if operation:
            confidence += 0.2
        
        # –ë–æ–Ω—É—Å—ã
        if len(text.split()) > 3:
            confidence += 0.05
        if any(keyword in text.lower() for keyword in ['—Å–¥–µ–ª–∫–∞', '–ø–æ–∑–∏—Ü–∏—è', '—Å–∏–≥–Ω–∞–ª']):
            confidence += 0.05
        
        return min(confidence, 1.0)

class MessageParsingService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –º–∞—Å—Å–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π"""
    
    def __init__(self, db_manager, parser: MessageParser = None):
        self.db = db_manager
        self.parser = parser or MessageParser()
    
    def parse_all_unprocessed_messages(self, limit: Optional[int] = None) -> Dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –Ω–µ—Ä–∞–∑–æ–±—Ä–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
        try:
            unprocessed = self._get_unprocessed_messages(limit)
            
            stats = {
                'total_processed': 0,
                'successful_parses': 0,
                'failed_parses': 0,
                'trading_messages': 0,
                'non_trading_messages': 0,
                'errors': []
            }
            
            logger.info(f"Starting to parse {len(unprocessed)} messages...")
            
            for message in unprocessed:
                try:
                    stats['total_processed'] += 1
                    
                    result = self.parser.parse_raw_message(message)
                    
                    if result.success:
                        signal_id = self.db.save_signal(result.signal_data)
                        if signal_id:
                            stats['successful_parses'] += 1
                            stats['trading_messages'] += 1
                        else:
                            stats['failed_parses'] += 1
                    else:
                        stats['failed_parses'] += 1
                        if result.error != "Not a trading message":
                            stats['errors'].append(f"Message {message['id']}: {result.error}")
                        else:
                            stats['non_trading_messages'] += 1
                    
                    self._mark_message_processed(message['id'])
                    
                    if stats['total_processed'] % 50 == 0:
                        logger.info(f"Processed {stats['total_processed']}/{len(unprocessed)}...")
                
                except Exception as e:
                    stats['failed_parses'] += 1
                    stats['errors'].append(f"Message {message['id']}: {str(e)}")
                    logger.error(f"Error processing message {message['id']}: {e}")
            
            logger.info(f"Parsing completed: {stats}")
            return stats
            
        except Exception as e:
            logger.error(f"Error in parse_all_unprocessed_messages: {e}")
            return {'error': str(e)}
    
    def _get_unprocessed_messages(self, limit: Optional[int] = None) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–µ—Ä–∞–∑–æ–±—Ä–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
        try:
            with self.db.session() as session:
                from core.database import RawMessage
                
                query = session.query(RawMessage).filter(
                    RawMessage.is_processed == False,
                    RawMessage.text.isnot(None),
                    RawMessage.text != ''
                ).order_by(RawMessage.timestamp)
                
                if limit:
                    query = query.limit(limit)
                
                messages = query.all()
                
                return [
                    {
                        'id': msg.id,
                        'text': msg.text,
                        'timestamp': msg.timestamp,
                        'channel_id': msg.channel_id,
                        'author_username': msg.author_username,
                        'message_id': msg.message_id
                    }
                    for msg in messages
                ]
        except Exception as e:
            logger.error(f"Error getting unprocessed messages: {e}")
            return []
    
    def _mark_message_processed(self, message_id: int) -> bool:
        """–ü–æ–º–µ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ"""
        try:
            with self.db.session() as session:
                from core.database import RawMessage
                
                message = session.query(RawMessage).filter(
                    RawMessage.id == message_id
                ).first()
                
                if message:
                    message.is_processed = True
                    message.processing_attempts = (message.processing_attempts or 0) + 1
                    return True
                
                return False
        except Exception as e:
            logger.error(f"Error marking message as processed: {e}")
            return False